{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook used to replicate the GRU model used for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The original publication is:\n",
    "##### Cho, K., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. https://doi.org/10.48550/arXiv.1406.1078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4986507513ba12be\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4986507513ba12be\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/GRU_1')\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GRU cell implementation\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_ir = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hr = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_iz = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hz = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_in = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hn = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        r = torch.sigmoid(self.W_ir(input) + self.W_hr(hidden))\n",
    "        z = torch.sigmoid(self.W_iz(input) + self.W_hz(hidden))\n",
    "        n = torch.tanh(self.W_in(input) + r * self.W_hn(hidden))\n",
    "        hidden_next = (1 - z) * n + z * hidden\n",
    "        return hidden_next\n",
    "\n",
    "# Encoder and Decoder with custom GRU cell\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru_cell = GRUCell(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        hidden = self.gru_cell(embedded, hidden)\n",
    "        return hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru_cell = GRUCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = torch.relu(output)\n",
    "        hidden = self.gru_cell(output, hidden)\n",
    "        output = self.softmax(self.out(hidden[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3165\n",
      "Epoch 2, Loss: 2.4133\n",
      "Epoch 3, Loss: 2.3337\n",
      "Epoch 4, Loss: 1.9344\n",
      "Epoch 5, Loss: 2.4961\n",
      "Epoch 6, Loss: 2.1230\n",
      "Epoch 7, Loss: 2.3756\n",
      "Epoch 8, Loss: 2.0419\n",
      "Epoch 9, Loss: 2.2257\n",
      "Epoch 10, Loss: 2.1194\n",
      "Epoch 11, Loss: 2.0342\n",
      "Epoch 12, Loss: 1.8471\n",
      "Epoch 13, Loss: 1.8658\n",
      "Epoch 14, Loss: 1.6227\n",
      "Epoch 15, Loss: 2.2170\n",
      "Epoch 16, Loss: 1.6445\n",
      "Epoch 17, Loss: 1.9594\n",
      "Epoch 18, Loss: 2.0793\n",
      "Epoch 19, Loss: 1.9135\n",
      "Epoch 20, Loss: 1.6522\n",
      "Epoch 21, Loss: 2.1921\n",
      "Epoch 22, Loss: 1.4983\n",
      "Epoch 23, Loss: 1.3690\n",
      "Epoch 24, Loss: 1.5570\n",
      "Epoch 25, Loss: 2.0763\n",
      "Epoch 26, Loss: 1.5262\n",
      "Epoch 27, Loss: 1.6865\n",
      "Epoch 28, Loss: 2.0822\n",
      "Epoch 29, Loss: 1.4526\n",
      "Epoch 30, Loss: 1.4956\n",
      "Epoch 31, Loss: 1.5620\n",
      "Epoch 32, Loss: 1.7888\n",
      "Epoch 33, Loss: 1.5322\n",
      "Epoch 34, Loss: 1.3774\n",
      "Epoch 35, Loss: 1.5272\n",
      "Epoch 36, Loss: 1.0284\n",
      "Epoch 37, Loss: 1.0567\n",
      "Epoch 38, Loss: 1.4462\n",
      "Epoch 39, Loss: 1.6150\n",
      "Epoch 40, Loss: 1.4043\n",
      "Epoch 41, Loss: 1.6527\n",
      "Epoch 42, Loss: 0.9711\n",
      "Epoch 43, Loss: 1.3923\n",
      "Epoch 44, Loss: 1.7341\n",
      "Epoch 45, Loss: 1.1076\n",
      "Epoch 46, Loss: 1.8892\n",
      "Epoch 47, Loss: 1.6389\n",
      "Epoch 48, Loss: 1.2315\n",
      "Epoch 49, Loss: 1.4116\n",
      "Epoch 50, Loss: 1.3880\n",
      "Epoch 51, Loss: 1.0517\n",
      "Epoch 52, Loss: 0.8582\n",
      "Epoch 53, Loss: 1.3222\n",
      "Epoch 54, Loss: 0.9717\n",
      "Epoch 55, Loss: 1.3565\n",
      "Epoch 56, Loss: 0.8167\n",
      "Epoch 57, Loss: 1.2212\n",
      "Epoch 58, Loss: 1.2875\n",
      "Epoch 59, Loss: 2.0852\n",
      "Epoch 60, Loss: 1.3217\n",
      "Epoch 61, Loss: 1.3382\n",
      "Epoch 62, Loss: 1.4349\n",
      "Epoch 63, Loss: 1.6531\n",
      "Epoch 64, Loss: 1.3404\n",
      "Epoch 65, Loss: 1.0003\n",
      "Epoch 66, Loss: 1.8441\n",
      "Epoch 67, Loss: 1.2138\n",
      "Epoch 68, Loss: 1.1665\n",
      "Epoch 69, Loss: 1.5681\n",
      "Epoch 70, Loss: 1.5362\n",
      "Epoch 71, Loss: 1.0320\n",
      "Epoch 72, Loss: 1.6221\n",
      "Epoch 73, Loss: 1.1908\n",
      "Epoch 74, Loss: 1.2849\n",
      "Epoch 75, Loss: 1.2680\n",
      "Epoch 76, Loss: 1.0018\n",
      "Epoch 77, Loss: 1.0585\n",
      "Epoch 78, Loss: 1.3767\n",
      "Epoch 79, Loss: 0.8542\n",
      "Epoch 80, Loss: 1.1021\n",
      "Epoch 81, Loss: 1.5116\n",
      "Epoch 82, Loss: 0.7466\n",
      "Epoch 83, Loss: 0.6909\n",
      "Epoch 84, Loss: 1.0499\n",
      "Epoch 85, Loss: 0.9154\n",
      "Epoch 86, Loss: 1.3945\n",
      "Epoch 87, Loss: 1.3502\n",
      "Epoch 88, Loss: 1.3419\n",
      "Epoch 89, Loss: 0.9457\n",
      "Epoch 90, Loss: 1.7166\n",
      "Epoch 91, Loss: 1.3768\n",
      "Epoch 92, Loss: 1.1678\n",
      "Epoch 93, Loss: 1.1297\n",
      "Epoch 94, Loss: 1.4263\n",
      "Epoch 95, Loss: 1.0066\n",
      "Epoch 96, Loss: 1.0159\n",
      "Epoch 97, Loss: 1.0126\n",
      "Epoch 98, Loss: 1.1597\n",
      "Epoch 99, Loss: 1.3993\n",
      "Epoch 100, Loss: 1.2307\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_size = 10  # Vocabulary size\n",
    "hidden_size = 16  # Size of hidden state\n",
    "output_size = 10  # Output vocabulary size\n",
    "max_length = 5  # Maximum sequence length\n",
    "\n",
    "# Instantiate the model\n",
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(100):\n",
    "    for _ in range(10):\n",
    "        # Generate a random sequence\n",
    "        input_seq = torch.tensor(np.random.choice(input_size, max_length), dtype=torch.long)\n",
    "        target = input_seq.numpy()[::-1].copy()\n",
    "        target_seq = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        # Initialize hidden state and loss\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        loss = 0\n",
    "\n",
    "        # Encoder\n",
    "        for i in range(input_seq.size(0)):\n",
    "            encoder_hidden = encoder(input_seq[i], encoder_hidden)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_input = torch.tensor([input_seq.size(0) - 1])  # Start token\n",
    "        decoder_hidden = encoder_hidden\n",
    "        for i in range(target_seq.size(0)):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_seq[i].unsqueeze(0))\n",
    "            decoder_input = target_seq[i]  # Teacher forcing\n",
    "\n",
    "        # Backpropagation\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item() / max_length:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5)\n",
    "encoder_hidden = encoder.initHidden()\n",
    "# encoder_hidden = encoder(input_seq[i], encoder_hidden)\n",
    "# print(hasattr(encoder, \"forward\"))\n",
    "writer.add_graph(encoder, (input_seq[i], encoder_hidden))\n",
    "# writer.add_graph(encoder,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [1 2 3 4 5]\n",
      "Output sequence: [5, 2, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    input_seq = torch.tensor([1, 2, 3, 4, 5], dtype=torch.long)\n",
    "    print(f'Input sequence: {input_seq.numpy()}')\n",
    "\n",
    "    # Encode the input sequence\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    for i in range(input_seq.size(0)):\n",
    "        encoder_hidden = encoder(input_seq[i], encoder_hidden)\n",
    "\n",
    "    # Decode the encoded sequence\n",
    "    decoder_input = torch.tensor([input_seq.size(0) - 1])  # Start token\n",
    "    decoder_hidden = encoder_hidden\n",
    "    output_seq = []\n",
    "    for i in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        output_seq.append(decoder_input.item())\n",
    "        if decoder_input.item() == 0:\n",
    "            break\n",
    "\n",
    "    print(f'Output sequence: {output_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
